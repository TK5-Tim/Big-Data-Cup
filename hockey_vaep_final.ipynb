{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Big Data Cup 2021 \n",
    "## How to value Zone Entries and other actions that are not shots or goals\n",
    "### VAEP (Valuing actions by estimating probabilities) framework for Hockey \n",
    "Inspired by paper of the Soccer version [Actions Speak Louder Than Goals: Valuing Player Actions in Soccer](https://arxiv.org/abs/1802.07127) by Tom Decroos, Lotte Bransen, Jan Van Haaren and Jesse Davis. Very helpful was the Tutorial as part of the Friends of Tracking initiative by Lotte Bransen and Jan Van Haaren: [Friends of Tracking: Valuing actions in football](https://github.com/SciSports-Labs/fot-valuing-actions)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 1;\n                var nbb_unformatted_code = \"%reload_ext nb_black\\nimport pandas as pd\\nfrom tqdm import tqdm\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.metrics import brier_score_loss, roc_auc_score\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\nfrom sklearn.calibration import CalibratedClassifierCV\\nfrom sklearn.model_selection import KFold\\nfrom xgboost import XGBClassifier, plot_importance\\n\\nimport shap\\nfrom ipywidgets import interact_manual, fixed, widgets\\n%matplotlib inline\";\n                var nbb_formatted_code = \"%reload_ext nb_black\\nimport pandas as pd\\nfrom tqdm import tqdm\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.metrics import brier_score_loss, roc_auc_score\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\nfrom sklearn.calibration import CalibratedClassifierCV\\nfrom sklearn.model_selection import KFold\\nfrom xgboost import XGBClassifier, plot_importance\\n\\nimport shap\\nfrom ipywidgets import interact_manual, fixed, widgets\\n\\n%matplotlib inline\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "%reload_ext nb_black\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import brier_score_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "\n",
    "import shap\n",
    "from ipywidgets import interact_manual, fixed, widgets\n",
    "%matplotlib inline"
   ]
  },
  {
   "source": [
    "# Importing data, renaming columns, creating extra columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 4;\n                var nbb_unformatted_code = \"# Import and Data Frame for womens data\\nproject_dir = '/Users/keltim01/git_repos/TK5/Data/Big-Data-Cup-2021/'\\nwomens = pd.read_csv(project_dir + 'hackathon_womens.csv')\\nnwhl = pd.read_csv(project_dir + 'hackathon_nwhl.csv')\\nwomens = womens.append(nwhl, ignore_index=True)\";\n                var nbb_formatted_code = \"# Import and Data Frame for womens data\\nproject_dir = \\\"/Users/keltim01/git_repos/TK5/Data/Big-Data-Cup-2021/\\\"\\nwomens = pd.read_csv(project_dir + \\\"hackathon_womens.csv\\\")\\nnwhl = pd.read_csv(project_dir + \\\"hackathon_nwhl.csv\\\")\\nwomens = womens.append(nwhl, ignore_index=True)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Import and Data Frame for womens data\n",
    "project_dir = '/Users/keltim01/git_repos/TK5/Data/Big-Data-Cup-2021/'\n",
    "womens = pd.read_csv(project_dir + 'hackathon_womens.csv')\n",
    "nwhl = pd.read_csv(project_dir + 'hackathon_nwhl.csv')\n",
    "womens = womens.append(nwhl, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 5;\n                var nbb_unformatted_code = \"# important numbers for the hockey rink \\nICE_LENGTH = 200\\nICE_WIDTH = 85\\nGOAL_X = ICE_LENGTH - 10\\nGOAL_Y = ICE_WIDTH / 2\\nD_ZONE = 75\\nO_ZONE = ICE_LENGTH - 75\";\n                var nbb_formatted_code = \"# important numbers for the hockey rink\\nICE_LENGTH = 200\\nICE_WIDTH = 85\\nGOAL_X = ICE_LENGTH - 10\\nGOAL_Y = ICE_WIDTH / 2\\nD_ZONE = 75\\nO_ZONE = ICE_LENGTH - 75\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# important numbers for the hockey rink \n",
    "ICE_LENGTH = 200\n",
    "ICE_WIDTH = 85\n",
    "GOAL_X = ICE_LENGTH - 10\n",
    "GOAL_Y = ICE_WIDTH / 2\n",
    "D_ZONE = 75\n",
    "O_ZONE = ICE_LENGTH - 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 6;\n                var nbb_unformatted_code = \"womens.columns = ['game_date', 'home_team', 'away_team', 'period', 'clock', 'home_team_skaters', 'away_team_skaters', 'home_team_goals','away_team_goals', 'team', 'player', 'event', 'x_coord', 'y_coord', 'detail_1', 'detail_2', 'detail_3', 'detail_4', 'player_2', 'x_coord_2', 'y_coord_2']\\nwomens['game_id'] = womens.loc[:, ['game_date', 'home_team', 'away_team']].sum(axis=1).astype('category').cat.codes\\nwomens['is_home'] = 0\\nwomens['is_shot'] = 0\\nwomens['is_goal'] = 0\\nwomens['event_id'] = womens['event'].astype('category').cat.codes\\nwomens['team_id'] = womens['team'].astype('category').cat.codes\\nwomens['player_id'] = womens['player'].astype('category').cat.codes\\n\\nfor x in range(1,5):\\n    womens[f'detail_{x}_code'] = womens[f'detail_{x}'].astype('category').cat.codes\\nwomens.loc[womens['home_team'] == womens['team'], 'is_home'] = 1\\nwomens.loc[womens['event']=='Shot', 'is_shot'] = 1\\nwomens.loc[womens['event']=='Goal', 'is_goal'] = 1\\nwomens['goal_diff'] = womens['home_team_goals'].sub(womens['away_team_goals'])\\nwomens['clock'] = pd.to_datetime(womens['clock'], format='%M:%S')\\nwomens['seconds_remaining'] = womens['clock'].dt.minute.mul(60).add(womens['clock'].dt.second)\";\n                var nbb_formatted_code = \"womens.columns = [\\n    \\\"game_date\\\",\\n    \\\"home_team\\\",\\n    \\\"away_team\\\",\\n    \\\"period\\\",\\n    \\\"clock\\\",\\n    \\\"home_team_skaters\\\",\\n    \\\"away_team_skaters\\\",\\n    \\\"home_team_goals\\\",\\n    \\\"away_team_goals\\\",\\n    \\\"team\\\",\\n    \\\"player\\\",\\n    \\\"event\\\",\\n    \\\"x_coord\\\",\\n    \\\"y_coord\\\",\\n    \\\"detail_1\\\",\\n    \\\"detail_2\\\",\\n    \\\"detail_3\\\",\\n    \\\"detail_4\\\",\\n    \\\"player_2\\\",\\n    \\\"x_coord_2\\\",\\n    \\\"y_coord_2\\\",\\n]\\nwomens[\\\"game_id\\\"] = (\\n    womens.loc[:, [\\\"game_date\\\", \\\"home_team\\\", \\\"away_team\\\"]]\\n    .sum(axis=1)\\n    .astype(\\\"category\\\")\\n    .cat.codes\\n)\\nwomens[\\\"is_home\\\"] = 0\\nwomens[\\\"is_shot\\\"] = 0\\nwomens[\\\"is_goal\\\"] = 0\\nwomens[\\\"event_id\\\"] = womens[\\\"event\\\"].astype(\\\"category\\\").cat.codes\\nwomens[\\\"team_id\\\"] = womens[\\\"team\\\"].astype(\\\"category\\\").cat.codes\\nwomens[\\\"player_id\\\"] = womens[\\\"player\\\"].astype(\\\"category\\\").cat.codes\\n\\nfor x in range(1, 5):\\n    womens[f\\\"detail_{x}_code\\\"] = womens[f\\\"detail_{x}\\\"].astype(\\\"category\\\").cat.codes\\nwomens.loc[womens[\\\"home_team\\\"] == womens[\\\"team\\\"], \\\"is_home\\\"] = 1\\nwomens.loc[womens[\\\"event\\\"] == \\\"Shot\\\", \\\"is_shot\\\"] = 1\\nwomens.loc[womens[\\\"event\\\"] == \\\"Goal\\\", \\\"is_goal\\\"] = 1\\nwomens[\\\"goal_diff\\\"] = womens[\\\"home_team_goals\\\"].sub(womens[\\\"away_team_goals\\\"])\\nwomens[\\\"clock\\\"] = pd.to_datetime(womens[\\\"clock\\\"], format=\\\"%M:%S\\\")\\nwomens[\\\"seconds_remaining\\\"] = (\\n    womens[\\\"clock\\\"].dt.minute.mul(60).add(womens[\\\"clock\\\"].dt.second)\\n)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "womens.columns = ['game_date', 'home_team', 'away_team', 'period', 'clock', 'home_team_skaters', 'away_team_skaters', 'home_team_goals','away_team_goals', 'team', 'player', 'event', 'x_coord', 'y_coord', 'detail_1', 'detail_2', 'detail_3', 'detail_4', 'player_2', 'x_coord_2', 'y_coord_2']\n",
    "womens['game_id'] = womens.loc[:, ['game_date', 'home_team', 'away_team']].sum(axis=1).astype('category').cat.codes\n",
    "womens['is_home'] = 0\n",
    "womens['is_shot'] = 0\n",
    "womens['is_goal'] = 0\n",
    "womens['event_id'] = womens['event'].astype('category').cat.codes\n",
    "womens['team_id'] = womens['team'].astype('category').cat.codes\n",
    "womens['player_id'] = womens['player'].astype('category').cat.codes\n",
    "\n",
    "for x in range(1,5):\n",
    "    womens[f'detail_{x}_code'] = womens[f'detail_{x}'].astype('category').cat.codes\n",
    "womens.loc[womens['home_team'] == womens['team'], 'is_home'] = 1\n",
    "womens.loc[womens['event']=='Shot', 'is_shot'] = 1\n",
    "womens.loc[womens['event']=='Goal', 'is_goal'] = 1\n",
    "womens['goal_diff'] = womens['home_team_goals'].sub(womens['away_team_goals'])\n",
    "womens['clock'] = pd.to_datetime(womens['clock'], format='%M:%S')\n",
    "womens['seconds_remaining'] = womens['clock'].dt.minute.mul(60).add(womens['clock'].dt.second)"
   ]
  },
  {
   "source": [
    "## Possession gained/lost\n",
    "* Shot: which team has the puck recovery? (next event)\n",
    "* Goal: not interesting because you scored -> 0\n",
    "* Play: possession stays -> 0\n",
    "* Incomplete Play: Possession lost -> -1\n",
    "* Takeaway: Possession won \n",
    "* Puck recovery: according to the team Possessing the puck before \n",
    "* Dump In/out: team recovering the puck \n",
    "* Zone Entry: \n",
    "    * carried: possesion retained \n",
    "    * dump in: next event \n",
    "    * passed: possesion retained \n",
    "* Faceoff Win: Possession gained\n",
    "* Penalty Taken: Possession 0 like goal\n",
    "\n",
    "## Glossary \n",
    "* -1 possesion lost through action \n",
    "* 0 possesion stays the same \n",
    "* 1 possesion gained through action"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 7;\n                var nbb_unformatted_code = \"womens.loc[(womens['event']=='Shot') & (womens['team']==womens['team'].shift(-1)),'poss_status'] = 0\\nwomens.loc[(womens['event']=='Shot') & (womens['team']!=womens['team'].shift(-1)),'poss_status'] = -1\\nwomens.loc[(womens['event']=='Puck Recovery') & (womens['team']!=womens['team'].shift(1)),'poss_status'] = 1\\nwomens.loc[(womens['event']=='Puck Recovery') & (womens['team']==womens['team'].shift(1)),'poss_status'] = 0\\nwomens.loc[(womens['event']=='Dump In/Out') & (womens['team']==womens['team'].shift(-1)),'poss_status'] = 0\\nwomens.loc[(womens['event']=='Dump In/Out') & (womens['team']!=womens['team'].shift(-1)),'poss_status'] = -1\\nwomens.loc[womens['event']=='Goal','poss_status'] = 0\\nwomens.loc[womens['event']=='Takeaway','poss_status'] = 1\\nwomens.loc[womens['event']=='Play','poss_status'] = 0\\nwomens.loc[womens['event']=='Incomplete Play','poss_status'] = -1\\nwomens.loc[(womens['event']=='Zone Entry') & (womens['detail_1']=='Carried'),'poss_status'] = 0\\nwomens.loc[(womens['event']=='Zone Entry') & (womens['detail_1']=='Passed'),'poss_status'] = 0\\nwomens.loc[(womens['event']=='Zone Entry') & (womens['detail_1']=='Dumped') & (womens['event'].shift(-1) == 'Faceoff Win'),'poss_status'] = 0\\nwomens.loc[(womens['event']=='Zone Entry') & (womens['detail_1']=='Dumped') & (womens['event'].shift(-1) == 'Penalty Taken'),'poss_status'] = 0\\nwomens.loc[(womens['team']==womens['team'].shift(-1)) & (womens['event']=='Zone Entry') & (womens['detail_1']=='Dumped') & (womens['event'].shift(-1) == 'Puck Recovery'),'poss_status'] = 0\\nwomens.loc[(womens['team']!=womens['team'].shift(-1)) & (womens['event']=='Zone Entry') & (womens['detail_1']=='Dumped') & (womens['event'].shift(-1) == 'Puck Recovery'),'poss_status'] = -1\\nwomens.loc[womens['event']=='Faceoff Win','poss_status'] = 1\\nwomens.loc[womens['event']=='Penalty Taken','poss_status'] = 0\";\n                var nbb_formatted_code = \"womens.loc[\\n    (womens[\\\"event\\\"] == \\\"Shot\\\") & (womens[\\\"team\\\"] == womens[\\\"team\\\"].shift(-1)),\\n    \\\"poss_status\\\",\\n] = 0\\nwomens.loc[\\n    (womens[\\\"event\\\"] == \\\"Shot\\\") & (womens[\\\"team\\\"] != womens[\\\"team\\\"].shift(-1)),\\n    \\\"poss_status\\\",\\n] = -1\\nwomens.loc[\\n    (womens[\\\"event\\\"] == \\\"Puck Recovery\\\") & (womens[\\\"team\\\"] != womens[\\\"team\\\"].shift(1)),\\n    \\\"poss_status\\\",\\n] = 1\\nwomens.loc[\\n    (womens[\\\"event\\\"] == \\\"Puck Recovery\\\") & (womens[\\\"team\\\"] == womens[\\\"team\\\"].shift(1)),\\n    \\\"poss_status\\\",\\n] = 0\\nwomens.loc[\\n    (womens[\\\"event\\\"] == \\\"Dump In/Out\\\") & (womens[\\\"team\\\"] == womens[\\\"team\\\"].shift(-1)),\\n    \\\"poss_status\\\",\\n] = 0\\nwomens.loc[\\n    (womens[\\\"event\\\"] == \\\"Dump In/Out\\\") & (womens[\\\"team\\\"] != womens[\\\"team\\\"].shift(-1)),\\n    \\\"poss_status\\\",\\n] = -1\\nwomens.loc[womens[\\\"event\\\"] == \\\"Goal\\\", \\\"poss_status\\\"] = 0\\nwomens.loc[womens[\\\"event\\\"] == \\\"Takeaway\\\", \\\"poss_status\\\"] = 1\\nwomens.loc[womens[\\\"event\\\"] == \\\"Play\\\", \\\"poss_status\\\"] = 0\\nwomens.loc[womens[\\\"event\\\"] == \\\"Incomplete Play\\\", \\\"poss_status\\\"] = -1\\nwomens.loc[\\n    (womens[\\\"event\\\"] == \\\"Zone Entry\\\") & (womens[\\\"detail_1\\\"] == \\\"Carried\\\"), \\\"poss_status\\\"\\n] = 0\\nwomens.loc[\\n    (womens[\\\"event\\\"] == \\\"Zone Entry\\\") & (womens[\\\"detail_1\\\"] == \\\"Passed\\\"), \\\"poss_status\\\"\\n] = 0\\nwomens.loc[\\n    (womens[\\\"event\\\"] == \\\"Zone Entry\\\")\\n    & (womens[\\\"detail_1\\\"] == \\\"Dumped\\\")\\n    & (womens[\\\"event\\\"].shift(-1) == \\\"Faceoff Win\\\"),\\n    \\\"poss_status\\\",\\n] = 0\\nwomens.loc[\\n    (womens[\\\"event\\\"] == \\\"Zone Entry\\\")\\n    & (womens[\\\"detail_1\\\"] == \\\"Dumped\\\")\\n    & (womens[\\\"event\\\"].shift(-1) == \\\"Penalty Taken\\\"),\\n    \\\"poss_status\\\",\\n] = 0\\nwomens.loc[\\n    (womens[\\\"team\\\"] == womens[\\\"team\\\"].shift(-1))\\n    & (womens[\\\"event\\\"] == \\\"Zone Entry\\\")\\n    & (womens[\\\"detail_1\\\"] == \\\"Dumped\\\")\\n    & (womens[\\\"event\\\"].shift(-1) == \\\"Puck Recovery\\\"),\\n    \\\"poss_status\\\",\\n] = 0\\nwomens.loc[\\n    (womens[\\\"team\\\"] != womens[\\\"team\\\"].shift(-1))\\n    & (womens[\\\"event\\\"] == \\\"Zone Entry\\\")\\n    & (womens[\\\"detail_1\\\"] == \\\"Dumped\\\")\\n    & (womens[\\\"event\\\"].shift(-1) == \\\"Puck Recovery\\\"),\\n    \\\"poss_status\\\",\\n] = -1\\nwomens.loc[womens[\\\"event\\\"] == \\\"Faceoff Win\\\", \\\"poss_status\\\"] = 1\\nwomens.loc[womens[\\\"event\\\"] == \\\"Penalty Taken\\\", \\\"poss_status\\\"] = 0\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "womens.loc[(womens['event']=='Shot') & (womens['team']==womens['team'].shift(-1)),'poss_status'] = 0\n",
    "womens.loc[(womens['event']=='Shot') & (womens['team']!=womens['team'].shift(-1)),'poss_status'] = -1\n",
    "womens.loc[(womens['event']=='Puck Recovery') & (womens['team']!=womens['team'].shift(1)),'poss_status'] = 1\n",
    "womens.loc[(womens['event']=='Puck Recovery') & (womens['team']==womens['team'].shift(1)),'poss_status'] = 0\n",
    "womens.loc[(womens['event']=='Dump In/Out') & (womens['team']==womens['team'].shift(-1)),'poss_status'] = 0\n",
    "womens.loc[(womens['event']=='Dump In/Out') & (womens['team']!=womens['team'].shift(-1)),'poss_status'] = -1\n",
    "womens.loc[womens['event']=='Goal','poss_status'] = 0\n",
    "womens.loc[womens['event']=='Takeaway','poss_status'] = 1\n",
    "womens.loc[womens['event']=='Play','poss_status'] = 0\n",
    "womens.loc[womens['event']=='Incomplete Play','poss_status'] = -1\n",
    "womens.loc[(womens['event']=='Zone Entry') & (womens['detail_1']=='Carried'),'poss_status'] = 0\n",
    "womens.loc[(womens['event']=='Zone Entry') & (womens['detail_1']=='Passed'),'poss_status'] = 0\n",
    "womens.loc[(womens['event']=='Zone Entry') & (womens['detail_1']=='Dumped') & (womens['event'].shift(-1) == 'Faceoff Win'),'poss_status'] = 0\n",
    "womens.loc[(womens['event']=='Zone Entry') & (womens['detail_1']=='Dumped') & (womens['event'].shift(-1) == 'Penalty Taken'),'poss_status'] = 0\n",
    "womens.loc[(womens['team']==womens['team'].shift(-1)) & (womens['event']=='Zone Entry') & (womens['detail_1']=='Dumped') & (womens['event'].shift(-1) == 'Puck Recovery'),'poss_status'] = 0\n",
    "womens.loc[(womens['team']!=womens['team'].shift(-1)) & (womens['event']=='Zone Entry') & (womens['detail_1']=='Dumped') & (womens['event'].shift(-1) == 'Puck Recovery'),'poss_status'] = -1\n",
    "womens.loc[womens['event']=='Faceoff Win','poss_status'] = 1\n",
    "womens.loc[womens['event']=='Penalty Taken','poss_status'] = 0"
   ]
  },
  {
   "source": [
    "## Strength States: differences between team Strengths \n",
    "* 2: possessing team has two skaters more than the opponent \n",
    "* 1: possessing team has one skater more than the opponent \n",
    "* 0: both teams have an equal number of skaters \n",
    "* -1: possessing team has one skater less than the opponent \n",
    "* -2: possessing team has two skaters less than the opponent \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 8;\n                var nbb_unformatted_code = \"womens.loc[womens['team']==womens['home_team'],'strength_state'] = womens.loc[womens['team']==womens['home_team'],'home_team_skaters'].sub(womens.loc[womens['team']==womens['home_team'],'away_team_skaters'])\\nwomens.loc[womens['team']==womens['away_team'],'strength_state'] = womens.loc[womens['team']==womens['away_team'],'away_team_skaters'].sub(womens.loc[womens['team']==womens['away_team'],'home_team_skaters'])\";\n                var nbb_formatted_code = \"womens.loc[womens[\\\"team\\\"] == womens[\\\"home_team\\\"], \\\"strength_state\\\"] = womens.loc[\\n    womens[\\\"team\\\"] == womens[\\\"home_team\\\"], \\\"home_team_skaters\\\"\\n].sub(womens.loc[womens[\\\"team\\\"] == womens[\\\"home_team\\\"], \\\"away_team_skaters\\\"])\\nwomens.loc[womens[\\\"team\\\"] == womens[\\\"away_team\\\"], \\\"strength_state\\\"] = womens.loc[\\n    womens[\\\"team\\\"] == womens[\\\"away_team\\\"], \\\"away_team_skaters\\\"\\n].sub(womens.loc[womens[\\\"team\\\"] == womens[\\\"away_team\\\"], \\\"home_team_skaters\\\"])\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "womens.loc[womens['team']==womens['home_team'],'strength_state'] = womens.loc[womens['team']==womens['home_team'],'home_team_skaters'].sub(womens.loc[womens['team']==womens['home_team'],'away_team_skaters'])\n",
    "womens.loc[womens['team']==womens['away_team'],'strength_state'] = womens.loc[womens['team']==womens['away_team'],'away_team_skaters'].sub(womens.loc[womens['team']==womens['away_team'],'home_team_skaters'])"
   ]
  },
  {
   "source": [
    "##  Calculate differences in disctance for actions & create endpoint for actions\n",
    "### Shot\n",
    "* on net: position of the goal\n",
    "* missed/blocked possesion lost or Retained: location next event -> Puck Recovery\n",
    "### Goal \n",
    "* position of the goal\n",
    "### Takeaway\n",
    "* same position\n",
    "### Puck Recovery\n",
    "* same position\n",
    "### Dump In/Out\n",
    "* Possession Lost or Retained: location of the next event (Puck Recovery)\n",
    "### Zone Entry\n",
    "* Carried: same position\n",
    "* Dumped: location of the next event (Puck Recovery)\n",
    "* Passed: entpoint of the pass\n",
    "### Faceoff Wins\n",
    "* same position\n",
    "### Penalty Taken\n",
    "* same position"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 10;\n                var nbb_unformatted_code = \"womens.loc[(womens['event']=='Shot') & (womens['detail_2'] == 'On Net'),['x_coord_2','y_coord_2']] = [GOAL_X,GOAL_Y]\\nshifted_coords = womens.loc[:,['x_coord','y_coord']].shift(-1)\\nwomens2 = womens.loc[:]\\nwomens2.loc[:,['x_coord','y_coord']] = shifted_coords\\nwomens.loc[(womens['event']=='Shot') & (womens['detail_2'] == 'Blocked'),'x_coord_2'] = womens2.loc[(womens2['event']=='Shot') & (womens2['detail_2'] == 'Blocked'),'x_coord']\\nwomens.loc[(womens['event']=='Shot') & (womens['detail_2'] == 'Blocked'),'y_coord_2'] = womens2.loc[(womens2['event']=='Shot') & (womens2['detail_2'] == 'Blocked'),'y_coord']\\nwomens.loc[(womens['event']=='Shot') & (womens['detail_2'] == 'Missed'),'x_coord_2'] = womens2.loc[(womens2['event']=='Shot') & (womens2['detail_2'] == 'Missed'),'x_coord']\\nwomens.loc[(womens['event']=='Shot') & (womens['detail_2'] == 'Missed'),'y_coord_2'] = womens2.loc[(womens2['event']=='Shot') & (womens2['detail_2'] == 'Missed'),'y_coord']\\nwomens.loc[womens['event']=='Goal',['x_coord_2','y_coord_2']] = [GOAL_X,GOAL_Y]\\nwomens.loc[womens['event']=='Takeaway','x_coord_2'] = womens.loc[womens['event']=='Takeaway','x_coord']\\nwomens.loc[womens['event']=='Takeaway','y_coord_2'] = womens.loc[womens['event']=='Takeaway','y_coord']\\nwomens.loc[womens['event']=='Puck Recovery','x_coord_2'] = womens.loc[womens['event']=='Puck Recovery','x_coord']\\nwomens.loc[womens['event']=='Puck Recovery','y_coord_2'] = womens.loc[womens['event']=='Puck Recovery','y_coord']\\nwomens.loc[womens['event']=='Dump In/Out','x_coord_2'] = womens2.loc[womens2['event']=='Dump In/Out','x_coord']\\nwomens.loc[womens['event']=='Dump In/Out','y_coord_2'] = womens2.loc[womens2['event']=='Dump In/Out','y_coord']\\nwomens.loc[womens['event']=='Zone Entry','x_coord_2'] = womens.loc[womens['event']=='Zone Entry','x_coord']\\nwomens.loc[womens['event']=='Zone Entry','y_coord_2'] = womens.loc[womens['event']=='Zone Entry','y_coord']\\nwomens.loc[(womens['event']=='Zone Entry') & (womens['detail_1']=='Dumped'),'x_coord_2'] = womens2.loc[(womens2['event']=='Zone Entry') & (womens2['detail_1']=='Dumped'),'x_coord']\\nwomens.loc[(womens['event']=='Zone Entry') & (womens['detail_1']=='Dumped'),'y_coord_2'] = womens2.loc[(womens2['event']=='Zone Entry') & (womens2['detail_1']=='Dumped'),'y_coord']\\nwomens.loc[womens['event']=='Faceoff Win','x_coord_2'] = womens.loc[womens['event']=='Faceoff Win','x_coord']\\nwomens.loc[womens['event']=='Faceoff Win','y_coord_2'] = womens.loc[womens['event']=='Faceoff Win','y_coord']\\nwomens.loc[womens['event']=='Penalty Taken','x_coord_2'] = womens.loc[womens['event']=='Penalty Taken','x_coord']\\nwomens.loc[womens['event']=='Penalty Taken','y_coord_2'] = womens.loc[womens['event']=='Penalty Taken','y_coord']\";\n                var nbb_formatted_code = \"womens.loc[\\n    (womens[\\\"event\\\"] == \\\"Shot\\\") & (womens[\\\"detail_2\\\"] == \\\"On Net\\\"),\\n    [\\\"x_coord_2\\\", \\\"y_coord_2\\\"],\\n] = [GOAL_X, GOAL_Y]\\nshifted_coords = womens.loc[:, [\\\"x_coord\\\", \\\"y_coord\\\"]].shift(-1)\\nwomens2 = womens.loc[:]\\nwomens2.loc[:, [\\\"x_coord\\\", \\\"y_coord\\\"]] = shifted_coords\\nwomens.loc[\\n    (womens[\\\"event\\\"] == \\\"Shot\\\") & (womens[\\\"detail_2\\\"] == \\\"Blocked\\\"), \\\"x_coord_2\\\"\\n] = womens2.loc[\\n    (womens2[\\\"event\\\"] == \\\"Shot\\\") & (womens2[\\\"detail_2\\\"] == \\\"Blocked\\\"), \\\"x_coord\\\"\\n]\\nwomens.loc[\\n    (womens[\\\"event\\\"] == \\\"Shot\\\") & (womens[\\\"detail_2\\\"] == \\\"Blocked\\\"), \\\"y_coord_2\\\"\\n] = womens2.loc[\\n    (womens2[\\\"event\\\"] == \\\"Shot\\\") & (womens2[\\\"detail_2\\\"] == \\\"Blocked\\\"), \\\"y_coord\\\"\\n]\\nwomens.loc[\\n    (womens[\\\"event\\\"] == \\\"Shot\\\") & (womens[\\\"detail_2\\\"] == \\\"Missed\\\"), \\\"x_coord_2\\\"\\n] = womens2.loc[\\n    (womens2[\\\"event\\\"] == \\\"Shot\\\") & (womens2[\\\"detail_2\\\"] == \\\"Missed\\\"), \\\"x_coord\\\"\\n]\\nwomens.loc[\\n    (womens[\\\"event\\\"] == \\\"Shot\\\") & (womens[\\\"detail_2\\\"] == \\\"Missed\\\"), \\\"y_coord_2\\\"\\n] = womens2.loc[\\n    (womens2[\\\"event\\\"] == \\\"Shot\\\") & (womens2[\\\"detail_2\\\"] == \\\"Missed\\\"), \\\"y_coord\\\"\\n]\\nwomens.loc[womens[\\\"event\\\"] == \\\"Goal\\\", [\\\"x_coord_2\\\", \\\"y_coord_2\\\"]] = [GOAL_X, GOAL_Y]\\nwomens.loc[womens[\\\"event\\\"] == \\\"Takeaway\\\", \\\"x_coord_2\\\"] = womens.loc[\\n    womens[\\\"event\\\"] == \\\"Takeaway\\\", \\\"x_coord\\\"\\n]\\nwomens.loc[womens[\\\"event\\\"] == \\\"Takeaway\\\", \\\"y_coord_2\\\"] = womens.loc[\\n    womens[\\\"event\\\"] == \\\"Takeaway\\\", \\\"y_coord\\\"\\n]\\nwomens.loc[womens[\\\"event\\\"] == \\\"Puck Recovery\\\", \\\"x_coord_2\\\"] = womens.loc[\\n    womens[\\\"event\\\"] == \\\"Puck Recovery\\\", \\\"x_coord\\\"\\n]\\nwomens.loc[womens[\\\"event\\\"] == \\\"Puck Recovery\\\", \\\"y_coord_2\\\"] = womens.loc[\\n    womens[\\\"event\\\"] == \\\"Puck Recovery\\\", \\\"y_coord\\\"\\n]\\nwomens.loc[womens[\\\"event\\\"] == \\\"Dump In/Out\\\", \\\"x_coord_2\\\"] = womens2.loc[\\n    womens2[\\\"event\\\"] == \\\"Dump In/Out\\\", \\\"x_coord\\\"\\n]\\nwomens.loc[womens[\\\"event\\\"] == \\\"Dump In/Out\\\", \\\"y_coord_2\\\"] = womens2.loc[\\n    womens2[\\\"event\\\"] == \\\"Dump In/Out\\\", \\\"y_coord\\\"\\n]\\nwomens.loc[womens[\\\"event\\\"] == \\\"Zone Entry\\\", \\\"x_coord_2\\\"] = womens.loc[\\n    womens[\\\"event\\\"] == \\\"Zone Entry\\\", \\\"x_coord\\\"\\n]\\nwomens.loc[womens[\\\"event\\\"] == \\\"Zone Entry\\\", \\\"y_coord_2\\\"] = womens.loc[\\n    womens[\\\"event\\\"] == \\\"Zone Entry\\\", \\\"y_coord\\\"\\n]\\nwomens.loc[\\n    (womens[\\\"event\\\"] == \\\"Zone Entry\\\") & (womens[\\\"detail_1\\\"] == \\\"Dumped\\\"), \\\"x_coord_2\\\"\\n] = womens2.loc[\\n    (womens2[\\\"event\\\"] == \\\"Zone Entry\\\") & (womens2[\\\"detail_1\\\"] == \\\"Dumped\\\"), \\\"x_coord\\\"\\n]\\nwomens.loc[\\n    (womens[\\\"event\\\"] == \\\"Zone Entry\\\") & (womens[\\\"detail_1\\\"] == \\\"Dumped\\\"), \\\"y_coord_2\\\"\\n] = womens2.loc[\\n    (womens2[\\\"event\\\"] == \\\"Zone Entry\\\") & (womens2[\\\"detail_1\\\"] == \\\"Dumped\\\"), \\\"y_coord\\\"\\n]\\nwomens.loc[womens[\\\"event\\\"] == \\\"Faceoff Win\\\", \\\"x_coord_2\\\"] = womens.loc[\\n    womens[\\\"event\\\"] == \\\"Faceoff Win\\\", \\\"x_coord\\\"\\n]\\nwomens.loc[womens[\\\"event\\\"] == \\\"Faceoff Win\\\", \\\"y_coord_2\\\"] = womens.loc[\\n    womens[\\\"event\\\"] == \\\"Faceoff Win\\\", \\\"y_coord\\\"\\n]\\nwomens.loc[womens[\\\"event\\\"] == \\\"Penalty Taken\\\", \\\"x_coord_2\\\"] = womens.loc[\\n    womens[\\\"event\\\"] == \\\"Penalty Taken\\\", \\\"x_coord\\\"\\n]\\nwomens.loc[womens[\\\"event\\\"] == \\\"Penalty Taken\\\", \\\"y_coord_2\\\"] = womens.loc[\\n    womens[\\\"event\\\"] == \\\"Penalty Taken\\\", \\\"y_coord\\\"\\n]\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "womens.loc[(womens['event']=='Shot') & (womens['detail_2'] == 'On Net'),['x_coord_2','y_coord_2']] = [GOAL_X,GOAL_Y]\n",
    "shifted_coords = womens.loc[:,['x_coord','y_coord']].shift(-1)\n",
    "womens2 = womens.loc[:]\n",
    "womens2.loc[:,['x_coord','y_coord']] = shifted_coords\n",
    "womens.loc[(womens['event']=='Shot') & (womens['detail_2'] == 'Blocked'),'x_coord_2'] = womens2.loc[(womens2['event']=='Shot') & (womens2['detail_2'] == 'Blocked'),'x_coord']\n",
    "womens.loc[(womens['event']=='Shot') & (womens['detail_2'] == 'Blocked'),'y_coord_2'] = womens2.loc[(womens2['event']=='Shot') & (womens2['detail_2'] == 'Blocked'),'y_coord']\n",
    "womens.loc[(womens['event']=='Shot') & (womens['detail_2'] == 'Missed'),'x_coord_2'] = womens2.loc[(womens2['event']=='Shot') & (womens2['detail_2'] == 'Missed'),'x_coord']\n",
    "womens.loc[(womens['event']=='Shot') & (womens['detail_2'] == 'Missed'),'y_coord_2'] = womens2.loc[(womens2['event']=='Shot') & (womens2['detail_2'] == 'Missed'),'y_coord']\n",
    "womens.loc[womens['event']=='Goal',['x_coord_2','y_coord_2']] = [GOAL_X,GOAL_Y]\n",
    "womens.loc[womens['event']=='Takeaway','x_coord_2'] = womens.loc[womens['event']=='Takeaway','x_coord']\n",
    "womens.loc[womens['event']=='Takeaway','y_coord_2'] = womens.loc[womens['event']=='Takeaway','y_coord']\n",
    "womens.loc[womens['event']=='Puck Recovery','x_coord_2'] = womens.loc[womens['event']=='Puck Recovery','x_coord']\n",
    "womens.loc[womens['event']=='Puck Recovery','y_coord_2'] = womens.loc[womens['event']=='Puck Recovery','y_coord']\n",
    "womens.loc[womens['event']=='Dump In/Out','x_coord_2'] = womens2.loc[womens2['event']=='Dump In/Out','x_coord']\n",
    "womens.loc[womens['event']=='Dump In/Out','y_coord_2'] = womens2.loc[womens2['event']=='Dump In/Out','y_coord']\n",
    "womens.loc[womens['event']=='Zone Entry','x_coord_2'] = womens.loc[womens['event']=='Zone Entry','x_coord']\n",
    "womens.loc[womens['event']=='Zone Entry','y_coord_2'] = womens.loc[womens['event']=='Zone Entry','y_coord']\n",
    "womens.loc[(womens['event']=='Zone Entry') & (womens['detail_1']=='Dumped'),'x_coord_2'] = womens2.loc[(womens2['event']=='Zone Entry') & (womens2['detail_1']=='Dumped'),'x_coord']\n",
    "womens.loc[(womens['event']=='Zone Entry') & (womens['detail_1']=='Dumped'),'y_coord_2'] = womens2.loc[(womens2['event']=='Zone Entry') & (womens2['detail_1']=='Dumped'),'y_coord']\n",
    "womens.loc[womens['event']=='Faceoff Win','x_coord_2'] = womens.loc[womens['event']=='Faceoff Win','x_coord']\n",
    "womens.loc[womens['event']=='Faceoff Win','y_coord_2'] = womens.loc[womens['event']=='Faceoff Win','y_coord']\n",
    "womens.loc[womens['event']=='Penalty Taken','x_coord_2'] = womens.loc[womens['event']=='Penalty Taken','x_coord']\n",
    "womens.loc[womens['event']=='Penalty Taken','y_coord_2'] = womens.loc[womens['event']=='Penalty Taken','y_coord']"
   ]
  },
  {
   "source": [
    "## make columns for in which zone a player is in and a diff column for it \n",
    "* 1 is the defensive zone \n",
    "* 2 is the neutral zone \n",
    "* 3 is the offensive zone\n",
    "* a positive difference is the difference in zones forward\n",
    "* a negative difference is the differene in zone backwards"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 11;\n                var nbb_unformatted_code = \"womens.loc[womens['x_coord'] <= D_ZONE, 'zone_1'] = 1\\nwomens.loc[womens['x_coord'] > D_ZONE, 'zone_1'] = 2\\nwomens.loc[womens['x_coord'] >= O_ZONE, 'zone_1'] = 3\\nwomens.loc[womens['x_coord_2'] <= D_ZONE, 'zone_2'] = 1\\nwomens.loc[womens['x_coord_2'] > D_ZONE, 'zone_2'] = 2\\nwomens.loc[womens['x_coord_2'] >= O_ZONE, 'zone_2'] = 3\\nwomens.loc[womens['event']=='Zone Entry','zone_1'] = 2\\nwomens.loc[womens['event']=='Zone Entry','zone_2'] = 3\\nwomens.loc[:,'zone_diff'] = womens['zone_2'] - womens['zone_1']\";\n                var nbb_formatted_code = \"womens.loc[womens[\\\"x_coord\\\"] <= D_ZONE, \\\"zone_1\\\"] = 1\\nwomens.loc[womens[\\\"x_coord\\\"] > D_ZONE, \\\"zone_1\\\"] = 2\\nwomens.loc[womens[\\\"x_coord\\\"] >= O_ZONE, \\\"zone_1\\\"] = 3\\nwomens.loc[womens[\\\"x_coord_2\\\"] <= D_ZONE, \\\"zone_2\\\"] = 1\\nwomens.loc[womens[\\\"x_coord_2\\\"] > D_ZONE, \\\"zone_2\\\"] = 2\\nwomens.loc[womens[\\\"x_coord_2\\\"] >= O_ZONE, \\\"zone_2\\\"] = 3\\nwomens.loc[womens[\\\"event\\\"] == \\\"Zone Entry\\\", \\\"zone_1\\\"] = 2\\nwomens.loc[womens[\\\"event\\\"] == \\\"Zone Entry\\\", \\\"zone_2\\\"] = 3\\nwomens.loc[:, \\\"zone_diff\\\"] = womens[\\\"zone_2\\\"] - womens[\\\"zone_1\\\"]\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "womens.loc[womens['x_coord'] <= D_ZONE, 'zone_1'] = 1\n",
    "womens.loc[womens['x_coord'] > D_ZONE, 'zone_1'] = 2\n",
    "womens.loc[womens['x_coord'] >= O_ZONE, 'zone_1'] = 3\n",
    "womens.loc[womens['x_coord_2'] <= D_ZONE, 'zone_2'] = 1\n",
    "womens.loc[womens['x_coord_2'] > D_ZONE, 'zone_2'] = 2\n",
    "womens.loc[womens['x_coord_2'] >= O_ZONE, 'zone_2'] = 3\n",
    "womens.loc[womens['event']=='Zone Entry','zone_1'] = 2\n",
    "womens.loc[womens['event']=='Zone Entry','zone_2'] = 3\n",
    "womens.loc[:,'zone_diff'] = womens['zone_2'] - womens['zone_1']"
   ]
  },
  {
   "source": [
    "## Calculate the distances and angles to the goals at the beginning and the end of the event"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 12;\n                var nbb_unformatted_code = \"diff_x1 = GOAL_X - womens['x_coord']\\ndiff_y1 = abs(GOAL_Y - womens['y_coord'])\\ndiff_x2 = GOAL_X - womens['x_coord_2']\\ndiff_y2 = abs(GOAL_Y - womens['y_coord_2'])\\nwomens['start_distance_to_goal'] = np.sqrt(diff_x1 ** 2 + diff_y1 ** 2)\\nwomens['end_distance_to_goal'] = np.sqrt(diff_x2 ** 2 + diff_y2 ** 2)\\nwomens['diff_x'] = womens['x_coord_2'] - womens['x_coord']\\nwomens['diff_y'] = womens['y_coord_2'] - womens['y_coord']\\nwomens['distance_covered'] = np.sqrt((womens['x_coord_2'] - womens['x_coord']) ** 2 + (womens['y_coord_2'] - womens['y_coord']) ** 2)\\ndiff_x1 = diff_x1.astype(float)\\nwomens['angle_to_goal_start'] = np.divide(diff_x1, diff_y1,out=np.zeros_like(diff_x1),where=(diff_y1 != 0))\\nwomens.loc[womens['angle_to_goal_start']>=360,'angle_to_goal_start'] = womens.loc[womens['angle_to_goal_start'] >=360,'angle_to_goal_start'] - 360\\nwomens.loc[womens['angle_to_goal_start']< 0,'angle_to_goal_start'] = womens.loc[womens['angle_to_goal_start'] < 0,'angle_to_goal_start'] + 360\\ndiff_x2 = diff_x2.astype(float)\\nwomens['angle_to_goal_end'] = np.divide(diff_x2, diff_y2,out=np.zeros_like(diff_x2),where=(diff_y2 != 0))\\nwomens.loc[womens['angle_to_goal_end']>=360,'angle_to_goal_end'] = womens.loc[womens['angle_to_goal_end'] >=360,'angle_to_goal_end'] - 360\\nwomens.loc[womens['angle_to_goal_end']< 0,'angle_to_goal_end'] = womens.loc[womens['angle_to_goal_end'] < 0,'angle_to_goal_end'] + 360\\nwomens['diff_angle_to_goal'] = womens['angle_to_goal_end'] - womens['angle_to_goal_start']\";\n                var nbb_formatted_code = \"diff_x1 = GOAL_X - womens[\\\"x_coord\\\"]\\ndiff_y1 = abs(GOAL_Y - womens[\\\"y_coord\\\"])\\ndiff_x2 = GOAL_X - womens[\\\"x_coord_2\\\"]\\ndiff_y2 = abs(GOAL_Y - womens[\\\"y_coord_2\\\"])\\nwomens[\\\"start_distance_to_goal\\\"] = np.sqrt(diff_x1 ** 2 + diff_y1 ** 2)\\nwomens[\\\"end_distance_to_goal\\\"] = np.sqrt(diff_x2 ** 2 + diff_y2 ** 2)\\nwomens[\\\"diff_x\\\"] = womens[\\\"x_coord_2\\\"] - womens[\\\"x_coord\\\"]\\nwomens[\\\"diff_y\\\"] = womens[\\\"y_coord_2\\\"] - womens[\\\"y_coord\\\"]\\nwomens[\\\"distance_covered\\\"] = np.sqrt(\\n    (womens[\\\"x_coord_2\\\"] - womens[\\\"x_coord\\\"]) ** 2\\n    + (womens[\\\"y_coord_2\\\"] - womens[\\\"y_coord\\\"]) ** 2\\n)\\ndiff_x1 = diff_x1.astype(float)\\nwomens[\\\"angle_to_goal_start\\\"] = np.divide(\\n    diff_x1, diff_y1, out=np.zeros_like(diff_x1), where=(diff_y1 != 0)\\n)\\nwomens.loc[womens[\\\"angle_to_goal_start\\\"] >= 360, \\\"angle_to_goal_start\\\"] = (\\n    womens.loc[womens[\\\"angle_to_goal_start\\\"] >= 360, \\\"angle_to_goal_start\\\"] - 360\\n)\\nwomens.loc[womens[\\\"angle_to_goal_start\\\"] < 0, \\\"angle_to_goal_start\\\"] = (\\n    womens.loc[womens[\\\"angle_to_goal_start\\\"] < 0, \\\"angle_to_goal_start\\\"] + 360\\n)\\ndiff_x2 = diff_x2.astype(float)\\nwomens[\\\"angle_to_goal_end\\\"] = np.divide(\\n    diff_x2, diff_y2, out=np.zeros_like(diff_x2), where=(diff_y2 != 0)\\n)\\nwomens.loc[womens[\\\"angle_to_goal_end\\\"] >= 360, \\\"angle_to_goal_end\\\"] = (\\n    womens.loc[womens[\\\"angle_to_goal_end\\\"] >= 360, \\\"angle_to_goal_end\\\"] - 360\\n)\\nwomens.loc[womens[\\\"angle_to_goal_end\\\"] < 0, \\\"angle_to_goal_end\\\"] = (\\n    womens.loc[womens[\\\"angle_to_goal_end\\\"] < 0, \\\"angle_to_goal_end\\\"] + 360\\n)\\nwomens[\\\"diff_angle_to_goal\\\"] = (\\n    womens[\\\"angle_to_goal_end\\\"] - womens[\\\"angle_to_goal_start\\\"]\\n)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "diff_x1 = GOAL_X - womens['x_coord']\n",
    "diff_y1 = abs(GOAL_Y - womens['y_coord'])\n",
    "diff_x2 = GOAL_X - womens['x_coord_2']\n",
    "diff_y2 = abs(GOAL_Y - womens['y_coord_2'])\n",
    "womens['start_distance_to_goal'] = np.sqrt(diff_x1 ** 2 + diff_y1 ** 2)\n",
    "womens['end_distance_to_goal'] = np.sqrt(diff_x2 ** 2 + diff_y2 ** 2)\n",
    "womens['diff_x'] = womens['x_coord_2'] - womens['x_coord']\n",
    "womens['diff_y'] = womens['y_coord_2'] - womens['y_coord']\n",
    "womens['distance_covered'] = np.sqrt((womens['x_coord_2'] - womens['x_coord']) ** 2 + (womens['y_coord_2'] - womens['y_coord']) ** 2)\n",
    "diff_x1 = diff_x1.astype(float)\n",
    "womens['angle_to_goal_start'] = np.divide(diff_x1, diff_y1,out=np.zeros_like(diff_x1),where=(diff_y1 != 0))\n",
    "womens.loc[womens['angle_to_goal_start']>=360,'angle_to_goal_start'] = womens.loc[womens['angle_to_goal_start'] >=360,'angle_to_goal_start'] - 360\n",
    "womens.loc[womens['angle_to_goal_start']< 0,'angle_to_goal_start'] = womens.loc[womens['angle_to_goal_start'] < 0,'angle_to_goal_start'] + 360\n",
    "diff_x2 = diff_x2.astype(float)\n",
    "womens['angle_to_goal_end'] = np.divide(diff_x2, diff_y2,out=np.zeros_like(diff_x2),where=(diff_y2 != 0))\n",
    "womens.loc[womens['angle_to_goal_end']>=360,'angle_to_goal_end'] = womens.loc[womens['angle_to_goal_end'] >=360,'angle_to_goal_end'] - 360\n",
    "womens.loc[womens['angle_to_goal_end']< 0,'angle_to_goal_end'] = womens.loc[womens['angle_to_goal_end'] < 0,'angle_to_goal_end'] + 360\n",
    "womens['diff_angle_to_goal'] = womens['angle_to_goal_end'] - womens['angle_to_goal_start']"
   ]
  },
  {
   "source": [
    "# non-shot xG models \n",
    "Assigns an xG value to every single event in the dataset even if it isn't a shot. The idea is to put a value on how dangerous a shot would be if taken in that position. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 13;\n                var nbb_unformatted_code = \"xg_features = ['x_coord','y_coord','start_distance_to_goal','angle_to_goal_start','strength_state']\\nxg_labels = ['is_goal']\";\n                var nbb_formatted_code = \"xg_features = [\\n    \\\"x_coord\\\",\\n    \\\"y_coord\\\",\\n    \\\"start_distance_to_goal\\\",\\n    \\\"angle_to_goal_start\\\",\\n    \\\"strength_state\\\",\\n]\\nxg_labels = [\\\"is_goal\\\"]\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "xg_features = ['x_coord','y_coord','start_distance_to_goal','angle_to_goal_start','strength_state']\n",
    "xg_labels = ['is_goal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "arning_rate=0.01, max_depth=3, n_estimators=1000, nthread=4, objective=binary:logistic, seed=42; total time=   2.9s\n",
      "[21:18:55] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=1000, nthread=4, objective=binary:logistic, seed=42; total time=   2.8s\n",
      "[21:18:57] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=1000, nthread=4, objective=binary:logistic, seed=42; total time=   3.0s\n",
      "[21:19:00] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=1000, nthread=4, objective=binary:logistic, seed=42; total time=   3.0s\n",
      "[21:19:03] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=4, n_estimators=100, nthread=4, objective=binary:logistic, seed=42; total time=   0.3s\n",
      "[21:19:04] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=4, n_estimators=100, nthread=4, objective=binary:logistic, seed=42; total time=   0.3s\n",
      "[21:19:04] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=4, n_estimators=100, nthread=4, objective=binary:logistic, seed=42; total time=   0.3s\n",
      "[21:19:04] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=4, n_estimators=100, nthread=4, objective=binary:logistic, seed=42; total time=   0.3s\n",
      "[21:19:05] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=4, n_estimators=100, nthread=4, objective=binary:logistic, seed=42; total time=   0.3s\n",
      "[21:19:05] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=4, n_estimators=500, nthread=4, objective=binary:logistic, seed=42; total time=   1.7s\n",
      "[21:19:07] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=4, n_estimators=500, nthread=4, objective=binary:logistic, seed=42; total time=   1.9s\n",
      "[21:19:09] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=4, n_estimators=500, nthread=4, objective=binary:logistic, seed=42; total time=   1.8s\n",
      "[21:19:11] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=4, n_estimators=500, nthread=4, objective=binary:logistic, seed=42; total time=   1.8s\n",
      "[21:19:12] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=4, n_estimators=500, nthread=4, objective=binary:logistic, seed=42; total time=   1.8s\n",
      "[21:19:14] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=4, n_estimators=1000, nthread=4, objective=binary:logistic, seed=42; total time=   3.4s\n",
      "[21:19:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=4, n_estimators=1000, nthread=4, objective=binary:logistic, seed=42; total time=   3.4s\n",
      "[21:19:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=4, n_estimators=1000, nthread=4, objective=binary:logistic, seed=42; total time=   3.3s\n",
      "[21:19:24] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=4, n_estimators=1000, nthread=4, objective=binary:logistic, seed=42; total time=   3.4s\n",
      "[21:19:28] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=4, n_estimators=1000, nthread=4, objective=binary:logistic, seed=42; total time=   3.4s\n",
      "[21:19:31] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=100, nthread=4, objective=binary:logistic, seed=42; total time=   0.4s\n",
      "[21:19:31] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=100, nthread=4, objective=binary:logistic, seed=42; total time=   0.3s\n",
      "[21:19:32] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=100, nthread=4, objective=binary:logistic, seed=42; total time=   0.4s\n",
      "[21:19:32] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=100, nthread=4, objective=binary:logistic, seed=42; total time=   0.4s\n",
      "[21:19:33] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=100, nthread=4, objective=binary:logistic, seed=42; total time=   0.3s\n",
      "[21:19:33] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=500, nthread=4, objective=binary:logistic, seed=42; total time=   1.8s\n",
      "[21:19:35] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=500, nthread=4, objective=binary:logistic, seed=42; total time=   1.8s\n",
      "[21:19:37] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=500, nthread=4, objective=binary:logistic, seed=42; total time=   1.8s\n",
      "[21:19:38] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=500, nthread=4, objective=binary:logistic, seed=42; total time=   1.9s\n",
      "[21:19:40] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=500, nthread=4, objective=binary:logistic, seed=42; total time=   1.9s\n",
      "[21:19:42] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=1000, nthread=4, objective=binary:logistic, seed=42; total time=   3.9s\n",
      "[21:19:46] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=1000, nthread=4, objective=binary:logistic, seed=42; total time=   3.9s\n",
      "[21:19:50] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=1000, nthread=4, objective=binary:logistic, seed=42; total time=   3.8s\n",
      "[21:19:54] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=1000, nthread=4, objective=binary:logistic, seed=42; total time=   4.0s\n",
      "[21:19:58] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=1000, nthread=4, objective=binary:logistic, seed=42; total time=   4.0s\n",
      "[21:20:02] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=6, n_estimators=100, nthread=4, objective=binary:logistic, seed=42; total time=   0.4s\n",
      "[21:20:02] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=6, n_estimators=100, nthread=4, objective=binary:logistic, seed=42; total time=   0.4s\n",
      "[21:20:03] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=6, n_estimators=100, nthread=4, objective=binary:logistic, seed=42; total time=   0.4s\n",
      "[21:20:03] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=6, n_estimators=100, nthread=4, objective=binary:logistic, seed=42; total time=   0.4s\n",
      "[21:20:03] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=6, n_estimators=100, nthread=4, objective=binary:logistic, seed=42; total time=   0.4s\n",
      "[21:20:04] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=6, n_estimators=500, nthread=4, objective=binary:logistic, seed=42; total time=   2.1s\n",
      "[21:20:06] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=6, n_estimators=500, nthread=4, objective=binary:logistic, seed=42; total time=   2.3s\n",
      "[21:20:08] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=6, n_estimators=500, nthread=4, objective=binary:logistic, seed=42; total time=   2.1s\n",
      "[21:20:10] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=6, n_estimators=500, nthread=4, objective=binary:logistic, seed=42; total time=   2.2s\n",
      "[21:20:12] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=6, n_estimators=500, nthread=4, objective=binary:logistic, seed=42; total time=   2.1s\n",
      "[21:20:15] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=6, n_estimators=1000, nthread=4, objective=binary:logistic, seed=42; total time=   4.5s\n",
      "[21:20:19] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=6, n_estimators=1000, nthread=4, objective=binary:logistic, seed=42; total time=   4.5s\n",
      "[21:20:23] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=6, n_estimators=1000, nthread=4, objective=binary:logistic, seed=42; total time=   4.3s\n",
      "[21:20:28] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=6, n_estimators=1000, nthread=4, objective=binary:logistic, seed=42; total time=   4.8s\n",
      "[21:20:33] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END learning_rate=0.01, max_depth=6, n_estimators=1000, nthread=4, objective=binary:logistic, seed=42; total time=   4.7s\n",
      "[21:20:37] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 14;\n                var nbb_unformatted_code = \"parameters = {\\n    'nthread': [4],\\n    'objective': ['binary:logistic'],\\n    'max_depth': [3, 4, 5, 6],\\n    'learning_rate': [0.01],\\n    'n_estimators': [100, 500, 1000],\\n    'seed': [42]\\n    }\\n\\ndf_xg_model = pd.DataFrame()\\nkf = KFold(10, shuffle=True)\\n\\nfor train_idx, test_idx in kf.split(womens):\\n    train_data = womens.iloc[train_idx].copy()\\n    test_data = womens.iloc[test_idx].copy()\\n\\n    classifier = XGBClassifier()\\n    classifier = GridSearchCV(classifier, parameters, scoring='roc_auc', verbose=2)\\n    classifier.fit(\\n        train_data[xg_features],\\n        train_data[xg_labels]\\n    )\\n    dfs_predictions = {}\\n    y_pred = classifier.predict_proba(test_data[xg_features])\\n    dfs_predictions[xg_labels[0]] = pd.Series(y_pred[:,1], index=test_data.index)\\n    df_predictions = pd.concat(dfs_predictions, axis=1)\\n    df_xg_model = df_xg_model.append(df_predictions)\";\n                var nbb_formatted_code = \"parameters = {\\n    \\\"nthread\\\": [4],\\n    \\\"objective\\\": [\\\"binary:logistic\\\"],\\n    \\\"max_depth\\\": [3, 4, 5, 6],\\n    \\\"learning_rate\\\": [0.01],\\n    \\\"n_estimators\\\": [100, 500, 1000],\\n    \\\"seed\\\": [42],\\n}\\n\\ndf_xg_model = pd.DataFrame()\\nkf = KFold(10, shuffle=True)\\n\\nfor train_idx, test_idx in kf.split(womens):\\n    train_data = womens.iloc[train_idx].copy()\\n    test_data = womens.iloc[test_idx].copy()\\n\\n    classifier = XGBClassifier()\\n    classifier = GridSearchCV(classifier, parameters, scoring=\\\"roc_auc\\\", verbose=2)\\n    classifier.fit(train_data[xg_features], train_data[xg_labels])\\n    dfs_predictions = {}\\n    y_pred = classifier.predict_proba(test_data[xg_features])\\n    dfs_predictions[xg_labels[0]] = pd.Series(y_pred[:, 1], index=test_data.index)\\n    df_predictions = pd.concat(dfs_predictions, axis=1)\\n    df_xg_model = df_xg_model.append(df_predictions)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "parameters = {\n",
    "    'nthread': [4],\n",
    "    'objective': ['binary:logistic'],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'learning_rate': [0.01],\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'seed': [42]\n",
    "    }\n",
    "\n",
    "df_xg_model = pd.DataFrame()\n",
    "kf = KFold(10, shuffle=True)\n",
    "\n",
    "for train_idx, test_idx in kf.split(womens):\n",
    "    train_data = womens.iloc[train_idx].copy()\n",
    "    test_data = womens.iloc[test_idx].copy()\n",
    "\n",
    "    classifier = XGBClassifier()\n",
    "    classifier = GridSearchCV(classifier, parameters, scoring='roc_auc', verbose=2)\n",
    "    classifier.fit(\n",
    "        train_data[xg_features],\n",
    "        train_data[xg_labels]\n",
    "    )\n",
    "    dfs_predictions = {}\n",
    "    y_pred = classifier.predict_proba(test_data[xg_features])\n",
    "    dfs_predictions[xg_labels[0]] = pd.Series(y_pred[:,1], index=test_data.index)\n",
    "    df_predictions = pd.concat(dfs_predictions, axis=1)\n",
    "    df_xg_model = df_xg_model.append(df_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 16;\n                var nbb_unformatted_code = \"womens['non_shot_xg'] = df_xg_model['is_goal']\";\n                var nbb_formatted_code = \"womens[\\\"non_shot_xg\\\"] = df_xg_model[\\\"is_goal\\\"]\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "womens['non_shot_xg'] = df_xg_model['is_goal']"
   ]
  },
  {
   "source": [
    "# VAEP - Model "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Features \n",
    "The features the model should use to predict a goal being scored or conceded. Like in the original vaep paper for soccer (linked in the introduction to the notebook) we are using the last 3 actions for predicting a score/concede in the next 10 actions. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 17;\n                var nbb_unformatted_code = \"features = ['game_id','team_id', 'player_id', 'period', 'x_coord', 'y_coord', 'x_coord_2',\\n       'y_coord_2', 'is_home', 'is_shot', 'is_goal', 'event_id',\\n       'goal_diff', 'seconds_remaining','diff_x', 'diff_y', 'distance_covered', 'start_distance_to_goal', 'end_distance_to_goal','zone_diff','poss_status','diff_angle_to_goal','non_shot_xg']\\ndf_delays = [womens[features].shift(step).add_suffix(f'-{step}') for step in range(0,3)]\\ndf_features = pd.concat(df_delays, axis=1)\";\n                var nbb_formatted_code = \"features = [\\n    \\\"game_id\\\",\\n    \\\"team_id\\\",\\n    \\\"player_id\\\",\\n    \\\"period\\\",\\n    \\\"x_coord\\\",\\n    \\\"y_coord\\\",\\n    \\\"x_coord_2\\\",\\n    \\\"y_coord_2\\\",\\n    \\\"is_home\\\",\\n    \\\"is_shot\\\",\\n    \\\"is_goal\\\",\\n    \\\"event_id\\\",\\n    \\\"goal_diff\\\",\\n    \\\"seconds_remaining\\\",\\n    \\\"diff_x\\\",\\n    \\\"diff_y\\\",\\n    \\\"distance_covered\\\",\\n    \\\"start_distance_to_goal\\\",\\n    \\\"end_distance_to_goal\\\",\\n    \\\"zone_diff\\\",\\n    \\\"poss_status\\\",\\n    \\\"diff_angle_to_goal\\\",\\n    \\\"non_shot_xg\\\",\\n]\\ndf_delays = [\\n    womens[features].shift(step).add_suffix(f\\\"-{step}\\\") for step in range(0, 3)\\n]\\ndf_features = pd.concat(df_delays, axis=1)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "features = ['game_id','team_id', 'player_id', 'period', 'x_coord', 'y_coord', 'x_coord_2',\n",
    "       'y_coord_2', 'is_home', 'is_shot', 'is_goal', 'event_id',\n",
    "       'goal_diff', 'seconds_remaining','diff_x', 'diff_y', 'distance_covered', 'start_distance_to_goal', 'end_distance_to_goal','zone_diff','poss_status','diff_angle_to_goal','non_shot_xg']\n",
    "df_delays = [womens[features].shift(step).add_suffix(f'-{step}') for step in range(0,3)]\n",
    "df_features = pd.concat(df_delays, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 18;\n                var nbb_unformatted_code = \"for step in range(0,3):\\n    df_features[f'team-{step}'] = df_features['team_id-0'] == df_features[f'team_id-{step}']\\n\\nfor step in range(0,3):\\n    df_features.loc[~(df_features[f'team-{step}']),f'x_coord-{step}'] = ICE_LENGTH - df_features[f'x_coord-{step}']\\n    df_features.loc[~(df_features[f'team-{step}']),f'x_coord_2-{step}'] = ICE_LENGTH - df_features[f'x_coord_2-{step}']\\n    df_features.loc[~(df_features[f'team-{step}']),f'y_coord-{step}'] = ICE_WIDTH - df_features[f'y_coord-{step}']\\n    df_features.loc[~(df_features[f'team-{step}']),f'y_coord_2-{step}'] = ICE_WIDTH - df_features[f'y_coord_2-{step}']\";\n                var nbb_formatted_code = \"for step in range(0, 3):\\n    df_features[f\\\"team-{step}\\\"] = (\\n        df_features[\\\"team_id-0\\\"] == df_features[f\\\"team_id-{step}\\\"]\\n    )\\n\\nfor step in range(0, 3):\\n    df_features.loc[~(df_features[f\\\"team-{step}\\\"]), f\\\"x_coord-{step}\\\"] = (\\n        ICE_LENGTH - df_features[f\\\"x_coord-{step}\\\"]\\n    )\\n    df_features.loc[~(df_features[f\\\"team-{step}\\\"]), f\\\"x_coord_2-{step}\\\"] = (\\n        ICE_LENGTH - df_features[f\\\"x_coord_2-{step}\\\"]\\n    )\\n    df_features.loc[~(df_features[f\\\"team-{step}\\\"]), f\\\"y_coord-{step}\\\"] = (\\n        ICE_WIDTH - df_features[f\\\"y_coord-{step}\\\"]\\n    )\\n    df_features.loc[~(df_features[f\\\"team-{step}\\\"]), f\\\"y_coord_2-{step}\\\"] = (\\n        ICE_WIDTH - df_features[f\\\"y_coord_2-{step}\\\"]\\n    )\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "for step in range(0,3):\n",
    "    df_features[f'team-{step}'] = df_features['team_id-0'] == df_features[f'team_id-{step}']\n",
    "\n",
    "for step in range(0,3):\n",
    "    df_features.loc[~(df_features[f'team-{step}']),f'x_coord-{step}'] = ICE_LENGTH - df_features[f'x_coord-{step}']\n",
    "    df_features.loc[~(df_features[f'team-{step}']),f'x_coord_2-{step}'] = ICE_LENGTH - df_features[f'x_coord_2-{step}']\n",
    "    df_features.loc[~(df_features[f'team-{step}']),f'y_coord-{step}'] = ICE_WIDTH - df_features[f'y_coord-{step}']\n",
    "    df_features.loc[~(df_features[f'team-{step}']),f'y_coord_2-{step}'] = ICE_WIDTH - df_features[f'y_coord_2-{step}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 19;\n                var nbb_unformatted_code = \"for step in range(0,3):\\n    start_diff_x = GOAL_X - df_features[f'x_coord-{step}']\\n    start_diff_y = abs(GOAL_Y - df_features[f'y_coord-{step}'])\\n    df_features[f'start_distance_to_goal-{step}'] = np.sqrt(start_diff_x ** 2 + start_diff_y ** 2)\\n    end_diff_x = GOAL_X - df_features[f'x_coord_2-{step}']\\n    end_diff_y = abs(GOAL_Y - df_features[f'y_coord_2-{step}'])\\n    df_features[f'end_distance_to_goal-{step}'] = np.sqrt(end_diff_x ** 2 + end_diff_y ** 2)\\n    df_features[f'diff_x-{step}'] = df_features[f'x_coord_2-{step}'] - df_features[f'x_coord-{step}']\\n    df_features[f'diff_y-{step}'] = df_features[f'y_coord_2-{step}'] - df_features[f'y_coord-{step}']\\n    df_features[f'distance_covered-{step}'] = np.sqrt((df_features[f'x_coord_2-{step}'] - df_features[f'x_coord-{step}']) ** 2 + (df_features[f'y_coord_2-{step}'] - df_features[f'y_coord-{step}']) ** 2)\";\n                var nbb_formatted_code = \"for step in range(0, 3):\\n    start_diff_x = GOAL_X - df_features[f\\\"x_coord-{step}\\\"]\\n    start_diff_y = abs(GOAL_Y - df_features[f\\\"y_coord-{step}\\\"])\\n    df_features[f\\\"start_distance_to_goal-{step}\\\"] = np.sqrt(\\n        start_diff_x ** 2 + start_diff_y ** 2\\n    )\\n    end_diff_x = GOAL_X - df_features[f\\\"x_coord_2-{step}\\\"]\\n    end_diff_y = abs(GOAL_Y - df_features[f\\\"y_coord_2-{step}\\\"])\\n    df_features[f\\\"end_distance_to_goal-{step}\\\"] = np.sqrt(\\n        end_diff_x ** 2 + end_diff_y ** 2\\n    )\\n    df_features[f\\\"diff_x-{step}\\\"] = (\\n        df_features[f\\\"x_coord_2-{step}\\\"] - df_features[f\\\"x_coord-{step}\\\"]\\n    )\\n    df_features[f\\\"diff_y-{step}\\\"] = (\\n        df_features[f\\\"y_coord_2-{step}\\\"] - df_features[f\\\"y_coord-{step}\\\"]\\n    )\\n    df_features[f\\\"distance_covered-{step}\\\"] = np.sqrt(\\n        (df_features[f\\\"x_coord_2-{step}\\\"] - df_features[f\\\"x_coord-{step}\\\"]) ** 2\\n        + (df_features[f\\\"y_coord_2-{step}\\\"] - df_features[f\\\"y_coord-{step}\\\"]) ** 2\\n    )\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "for step in range(0,3):\n",
    "    start_diff_x = GOAL_X - df_features[f'x_coord-{step}']\n",
    "    start_diff_y = abs(GOAL_Y - df_features[f'y_coord-{step}'])\n",
    "    df_features[f'start_distance_to_goal-{step}'] = np.sqrt(start_diff_x ** 2 + start_diff_y ** 2)\n",
    "    end_diff_x = GOAL_X - df_features[f'x_coord_2-{step}']\n",
    "    end_diff_y = abs(GOAL_Y - df_features[f'y_coord_2-{step}'])\n",
    "    df_features[f'end_distance_to_goal-{step}'] = np.sqrt(end_diff_x ** 2 + end_diff_y ** 2)\n",
    "    df_features[f'diff_x-{step}'] = df_features[f'x_coord_2-{step}'] - df_features[f'x_coord-{step}']\n",
    "    df_features[f'diff_y-{step}'] = df_features[f'y_coord_2-{step}'] - df_features[f'y_coord-{step}']\n",
    "    df_features[f'distance_covered-{step}'] = np.sqrt((df_features[f'x_coord_2-{step}'] - df_features[f'x_coord-{step}']) ** 2 + (df_features[f'y_coord_2-{step}'] - df_features[f'y_coord-{step}']) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 20;\n                var nbb_unformatted_code = \"df_features['xdiff_sequenc_pre'] = df_features['x_coord-0'] - df_features['x_coord-2']\\ndf_features['ydiff_sequenc_pre'] = df_features['y_coord-0'] - df_features['y_coord-2']\\ndf_features['time_sequence_pre'] = df_features['seconds_remaining-0'] - df_features['seconds_remaining-2']\";\n                var nbb_formatted_code = \"df_features[\\\"xdiff_sequenc_pre\\\"] = df_features[\\\"x_coord-0\\\"] - df_features[\\\"x_coord-2\\\"]\\ndf_features[\\\"ydiff_sequenc_pre\\\"] = df_features[\\\"y_coord-0\\\"] - df_features[\\\"y_coord-2\\\"]\\ndf_features[\\\"time_sequence_pre\\\"] = (\\n    df_features[\\\"seconds_remaining-0\\\"] - df_features[\\\"seconds_remaining-2\\\"]\\n)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "df_features['xdiff_sequenc_pre'] = df_features['x_coord-0'] - df_features['x_coord-2']\n",
    "df_features['ydiff_sequenc_pre'] = df_features['y_coord-0'] - df_features['y_coord-2']\n",
    "df_features['time_sequence_pre'] = df_features['seconds_remaining-0'] - df_features['seconds_remaining-2']"
   ]
  },
  {
   "source": [
    "## Labels \n",
    "the model should make the predictions on the labels scores and concedes for every action. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 21;\n                var nbb_unformatted_code = \"goals = womens['event'].str.contains('Goal')\\ny = pd.concat([womens.loc[:, 'is_goal'], womens.loc[:,'team_id']], axis = 1)\\ny.columns = ['goal','team_id']\\nfor i in range(1, 10):\\n    for col in ['team_id', 'goal']:\\n        shifted = y[col].shift(-i)\\n        shifted[-i:] = y[col][len(y) - 1]\\n        y[f'{col}+{i}'] = shifted.astype(int)\\n\\nscores = y['goal']\\nconcedes = y['goal']\\nfor i in range(1, 10):\\n    goal_scored = y[f'goal+{i}'] & (y[f'team_id+{i}'] == y['team_id'])\\n    goal_opponent = y[f'goal+{i}'] & (y[f'team_id+{i}'] != y['team_id'])\\n    scores = scores | goal_scored\\n    concedes = concedes | goal_opponent\\nlabel_scores = pd.DataFrame(scores, columns=['scores'])\\nlabel_concedes = pd.DataFrame(concedes, columns=['concedes'])\\ndf_labels = pd.concat([label_scores, label_concedes], axis=1)\";\n                var nbb_formatted_code = \"goals = womens[\\\"event\\\"].str.contains(\\\"Goal\\\")\\ny = pd.concat([womens.loc[:, \\\"is_goal\\\"], womens.loc[:, \\\"team_id\\\"]], axis=1)\\ny.columns = [\\\"goal\\\", \\\"team_id\\\"]\\nfor i in range(1, 10):\\n    for col in [\\\"team_id\\\", \\\"goal\\\"]:\\n        shifted = y[col].shift(-i)\\n        shifted[-i:] = y[col][len(y) - 1]\\n        y[f\\\"{col}+{i}\\\"] = shifted.astype(int)\\n\\nscores = y[\\\"goal\\\"]\\nconcedes = y[\\\"goal\\\"]\\nfor i in range(1, 10):\\n    goal_scored = y[f\\\"goal+{i}\\\"] & (y[f\\\"team_id+{i}\\\"] == y[\\\"team_id\\\"])\\n    goal_opponent = y[f\\\"goal+{i}\\\"] & (y[f\\\"team_id+{i}\\\"] != y[\\\"team_id\\\"])\\n    scores = scores | goal_scored\\n    concedes = concedes | goal_opponent\\nlabel_scores = pd.DataFrame(scores, columns=[\\\"scores\\\"])\\nlabel_concedes = pd.DataFrame(concedes, columns=[\\\"concedes\\\"])\\ndf_labels = pd.concat([label_scores, label_concedes], axis=1)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "goals = womens['event'].str.contains('Goal')\n",
    "y = pd.concat([womens.loc[:, 'is_goal'], womens.loc[:,'team_id']], axis = 1)\n",
    "y.columns = ['goal','team_id']\n",
    "for i in range(1, 10):\n",
    "    for col in ['team_id', 'goal']:\n",
    "        shifted = y[col].shift(-i)\n",
    "        shifted[-i:] = y[col][len(y) - 1]\n",
    "        y[f'{col}+{i}'] = shifted.astype(int)\n",
    "\n",
    "scores = y['goal']\n",
    "concedes = y['goal']\n",
    "for i in range(1, 10):\n",
    "    goal_scored = y[f'goal+{i}'] & (y[f'team_id+{i}'] == y['team_id'])\n",
    "    goal_opponent = y[f'goal+{i}'] & (y[f'team_id+{i}'] != y['team_id'])\n",
    "    scores = scores | goal_scored\n",
    "    concedes = concedes | goal_opponent\n",
    "label_scores = pd.DataFrame(scores, columns=['scores'])\n",
    "label_concedes = pd.DataFrame(concedes, columns=['concedes'])\n",
    "df_labels = pd.concat([label_scores, label_concedes], axis=1)"
   ]
  },
  {
   "source": [
    "## Modelling the score and concede values "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 22;\n                var nbb_unformatted_code = \"labels = ['scores','concedes']\\nfeat = ['start_distance_to_goal-0', 'end_distance_to_goal-0', 'start_distance_to_goal-1', 'end_distance_to_goal-1', 'start_distance_to_goal-2', 'end_distance_to_goal-2','team-1', 'team-2','seconds_remaining-0','goal_diff-0','zone_diff-0','zone_diff-1','zone_diff-2','poss_status-0','poss_status-1','poss_status-2', 'diff_angle_to_goal-0','diff_angle_to_goal-1','diff_angle_to_goal-2','non_shot_xg-0','non_shot_xg-1','non_shot_xg-2']\";\n                var nbb_formatted_code = \"labels = [\\\"scores\\\", \\\"concedes\\\"]\\nfeat = [\\n    \\\"start_distance_to_goal-0\\\",\\n    \\\"end_distance_to_goal-0\\\",\\n    \\\"start_distance_to_goal-1\\\",\\n    \\\"end_distance_to_goal-1\\\",\\n    \\\"start_distance_to_goal-2\\\",\\n    \\\"end_distance_to_goal-2\\\",\\n    \\\"team-1\\\",\\n    \\\"team-2\\\",\\n    \\\"seconds_remaining-0\\\",\\n    \\\"goal_diff-0\\\",\\n    \\\"zone_diff-0\\\",\\n    \\\"zone_diff-1\\\",\\n    \\\"zone_diff-2\\\",\\n    \\\"poss_status-0\\\",\\n    \\\"poss_status-1\\\",\\n    \\\"poss_status-2\\\",\\n    \\\"diff_angle_to_goal-0\\\",\\n    \\\"diff_angle_to_goal-1\\\",\\n    \\\"diff_angle_to_goal-2\\\",\\n    \\\"non_shot_xg-0\\\",\\n    \\\"non_shot_xg-1\\\",\\n    \\\"non_shot_xg-2\\\",\\n]\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "labels = ['scores','concedes']\n",
    "feat = ['start_distance_to_goal-0', 'end_distance_to_goal-0', 'start_distance_to_goal-1', 'end_distance_to_goal-1', 'start_distance_to_goal-2', 'end_distance_to_goal-2','team-1', 'team-2','seconds_remaining-0','goal_diff-0','zone_diff-0','zone_diff-1','zone_diff-2','poss_status-0','poss_status-1','poss_status-2', 'diff_angle_to_goal-0','diff_angle_to_goal-1','diff_angle_to_goal-2','non_shot_xg-0','non_shot_xg-1','non_shot_xg-2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s][21:33:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " 50%|     | 1/2 [00:01<00:01,  1.00s/it][21:33:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|| 2/2 [00:01<00:00,  1.04it/s]\n",
      "100%|| 2/2 [00:00<00:00, 67.53it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]0\n",
      "0\n",
      "[21:33:19] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " 50%|     | 1/2 [00:00<00:00,  1.03it/s][21:33:20] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|| 2/2 [00:01<00:00,  1.08it/s]\n",
      "100%|| 2/2 [00:00<00:00, 68.69it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]0\n",
      "0\n",
      "[21:33:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " 50%|     | 1/2 [00:00<00:00,  1.12it/s][21:33:21] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|| 2/2 [00:01<00:00,  1.15it/s]\n",
      "100%|| 2/2 [00:00<00:00, 80.78it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]0\n",
      "0\n",
      "[21:33:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " 50%|     | 1/2 [00:00<00:00,  1.09it/s][21:33:23] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|| 2/2 [00:01<00:00,  1.13it/s]\n",
      "100%|| 2/2 [00:00<00:00, 72.83it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]0\n",
      "0\n",
      "[21:33:24] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " 50%|     | 1/2 [00:00<00:00,  1.04it/s][21:33:25] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|| 2/2 [00:01<00:00,  1.06it/s]\n",
      "100%|| 2/2 [00:00<00:00, 72.49it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]0\n",
      "0\n",
      "[21:33:26] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " 50%|     | 1/2 [00:00<00:00,  1.03it/s][21:33:27] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|| 2/2 [00:01<00:00,  1.02it/s]\n",
      "100%|| 2/2 [00:00<00:00, 35.08it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]0\n",
      "0\n",
      "[21:33:28] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " 50%|     | 1/2 [00:00<00:00,  1.05it/s][21:33:29] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|| 2/2 [00:01<00:00,  1.06it/s]\n",
      "100%|| 2/2 [00:00<00:00, 71.61it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]0\n",
      "0\n",
      "[21:33:30] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " 50%|     | 1/2 [00:00<00:00,  1.03it/s][21:33:31] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|| 2/2 [00:01<00:00,  1.02it/s]\n",
      "100%|| 2/2 [00:00<00:00, 71.94it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]0\n",
      "0\n",
      "[21:33:32] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " 50%|     | 1/2 [00:01<00:01,  1.02s/it][21:33:33] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|| 2/2 [00:01<00:00,  1.02it/s]\n",
      "100%|| 2/2 [00:00<00:00, 69.10it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]0\n",
      "0\n",
      "[21:33:34] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " 50%|     | 1/2 [00:00<00:00,  1.11it/s][21:33:35] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|| 2/2 [00:01<00:00,  1.04it/s]\n",
      "100%|| 2/2 [00:00<00:00, 63.13it/s]0\n",
      "0\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 23;\n                var nbb_unformatted_code = \"df_model = pd.concat([df_features,df_labels],axis=1)\\ndf_score_concede_prob = pd.DataFrame()\\nkf = KFold(10, shuffle=True)\\n\\nfor train_idx, test_idx in kf.split(df_model):\\n    train_data = df_model.iloc[train_idx].copy()\\n    test_data = df_model.iloc[test_idx].copy()\\n\\n    models = {}\\n    for label in tqdm(labels):\\n        model = XGBClassifier(\\n            n_estimators=50,\\n            max_depth=3\\n        )\\n        model.fit(\\n            X=train_data[feat],\\n            y=train_data[label]\\n        )\\n        models[label] = model\\n\\n    dfs_predictions = {}\\n    for label in tqdm(labels):\\n        model = models[label]\\n        probabilities = model.predict_proba(test_data[feat])\\n        predictions = probabilities[:, 1]\\n        print(np.isnan(probabilities).sum())\\n        dfs_predictions[label] = pd.Series(predictions, index=test_data.index)\\n    df_predictions = pd.concat(dfs_predictions, axis=1)\\n    df_score_concede_prob = df_score_concede_prob.append(df_predictions)\";\n                var nbb_formatted_code = \"df_model = pd.concat([df_features, df_labels], axis=1)\\ndf_score_concede_prob = pd.DataFrame()\\nkf = KFold(10, shuffle=True)\\n\\nfor train_idx, test_idx in kf.split(df_model):\\n    train_data = df_model.iloc[train_idx].copy()\\n    test_data = df_model.iloc[test_idx].copy()\\n\\n    models = {}\\n    for label in tqdm(labels):\\n        model = XGBClassifier(n_estimators=50, max_depth=3)\\n        model.fit(X=train_data[feat], y=train_data[label])\\n        models[label] = model\\n\\n    dfs_predictions = {}\\n    for label in tqdm(labels):\\n        model = models[label]\\n        probabilities = model.predict_proba(test_data[feat])\\n        predictions = probabilities[:, 1]\\n        print(np.isnan(probabilities).sum())\\n        dfs_predictions[label] = pd.Series(predictions, index=test_data.index)\\n    df_predictions = pd.concat(dfs_predictions, axis=1)\\n    df_score_concede_prob = df_score_concede_prob.append(df_predictions)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "df_model = pd.concat([df_features,df_labels],axis=1)\n",
    "df_score_concede_prob = pd.DataFrame()\n",
    "kf = KFold(10, shuffle=True)\n",
    "\n",
    "for train_idx, test_idx in kf.split(df_model):\n",
    "    train_data = df_model.iloc[train_idx].copy()\n",
    "    test_data = df_model.iloc[test_idx].copy()\n",
    "\n",
    "    models = {}\n",
    "    for label in tqdm(labels):\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=50,\n",
    "            max_depth=3\n",
    "        )\n",
    "        model.fit(\n",
    "            X=train_data[feat],\n",
    "            y=train_data[label]\n",
    "        )\n",
    "        models[label] = model\n",
    "\n",
    "    dfs_predictions = {}\n",
    "    for label in tqdm(labels):\n",
    "        model = models[label]\n",
    "        probabilities = model.predict_proba(test_data[feat])\n",
    "        predictions = probabilities[:, 1]\n",
    "        print(np.isnan(probabilities).sum())\n",
    "        dfs_predictions[label] = pd.Series(predictions, index=test_data.index)\n",
    "    df_predictions = pd.concat(dfs_predictions, axis=1)\n",
    "    df_score_concede_prob = df_score_concede_prob.append(df_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 24;\n                var nbb_unformatted_code = \"dfs_actions = []\\ndfs_actions.append(womens)\\ndf_actions = pd.concat(dfs_actions).reset_index(drop=True)\\n\\ndf_actions_predictions = pd.concat([df_actions, df_score_concede_prob], axis=1)\\ndf_actions_predictions = df_actions_predictions.dropna(subset=['start_distance_to_goal', 'end_distance_to_goal', 'diff_x', 'diff_y',\\n       'distance_covered', 'scores', 'concedes'])\";\n                var nbb_formatted_code = \"dfs_actions = []\\ndfs_actions.append(womens)\\ndf_actions = pd.concat(dfs_actions).reset_index(drop=True)\\n\\ndf_actions_predictions = pd.concat([df_actions, df_score_concede_prob], axis=1)\\ndf_actions_predictions = df_actions_predictions.dropna(\\n    subset=[\\n        \\\"start_distance_to_goal\\\",\\n        \\\"end_distance_to_goal\\\",\\n        \\\"diff_x\\\",\\n        \\\"diff_y\\\",\\n        \\\"distance_covered\\\",\\n        \\\"scores\\\",\\n        \\\"concedes\\\",\\n    ]\\n)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "dfs_actions = []\n",
    "dfs_actions.append(womens)\n",
    "df_actions = pd.concat(dfs_actions).reset_index(drop=True)\n",
    "\n",
    "df_actions_predictions = pd.concat([df_actions, df_score_concede_prob], axis=1)\n",
    "df_actions_predictions = df_actions_predictions.dropna(subset=['start_distance_to_goal', 'end_distance_to_goal', 'diff_x', 'diff_y',\n",
    "       'distance_covered', 'scores', 'concedes'])"
   ]
  },
  {
   "source": [
    "## calculate the final VAEP value from score and conced"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 25;\n                var nbb_unformatted_code = \"def prev(x: pd.Series) -> pd.Series:\\n    prev_x = x.shift(1)\\n    prev_x[:1] = x.values[0]\\n    return prev_x\";\n                var nbb_formatted_code = \"def prev(x: pd.Series) -> pd.Series:\\n    prev_x = x.shift(1)\\n    prev_x[:1] = x.values[0]\\n    return prev_x\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "def prev(x: pd.Series) -> pd.Series:\n",
    "    prev_x = x.shift(1)\n",
    "    prev_x[:1] = x.values[0]\n",
    "    return prev_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 26;\n                var nbb_unformatted_code = \"dfs_values = []\\ndf_values = pd.DataFrame()\\n\\nsameteam = prev(df_actions_predictions.team_id) == df_actions_predictions.team_id\\nprev_scores = prev(df_actions_predictions.scores) * sameteam + prev(df_actions_predictions.concedes) * (~sameteam)\\nprev_concedes = prev(df_actions_predictions.concedes) * sameteam + prev(df_actions_predictions.scores) * (~sameteam)\\n\\ntoolong_idx = abs(prev(df_actions_predictions.seconds_remaining) - df_actions_predictions.seconds_remaining) > 10\\nprev_scores[toolong_idx] = 0\\nprev_concedes[toolong_idx] = 0\\n\\nprevgoal_idx = prev(df_actions_predictions.event) == 'Goal'\\nprev_scores[prevgoal_idx] = 0\\nprev_concedes[prevgoal_idx] = 0\\n\\ndf_values['offensive_value'] = df_actions_predictions.scores - prev_scores\\ndf_values['defensive_value'] = df_actions_predictions.concedes - prev_concedes\\ndf_values['vaep'] = df_values['offensive_value'] + df_values['defensive_value']\";\n                var nbb_formatted_code = \"dfs_values = []\\ndf_values = pd.DataFrame()\\n\\nsameteam = prev(df_actions_predictions.team_id) == df_actions_predictions.team_id\\nprev_scores = prev(df_actions_predictions.scores) * sameteam + prev(\\n    df_actions_predictions.concedes\\n) * (~sameteam)\\nprev_concedes = prev(df_actions_predictions.concedes) * sameteam + prev(\\n    df_actions_predictions.scores\\n) * (~sameteam)\\n\\ntoolong_idx = (\\n    abs(\\n        prev(df_actions_predictions.seconds_remaining)\\n        - df_actions_predictions.seconds_remaining\\n    )\\n    > 10\\n)\\nprev_scores[toolong_idx] = 0\\nprev_concedes[toolong_idx] = 0\\n\\nprevgoal_idx = prev(df_actions_predictions.event) == \\\"Goal\\\"\\nprev_scores[prevgoal_idx] = 0\\nprev_concedes[prevgoal_idx] = 0\\n\\ndf_values[\\\"offensive_value\\\"] = df_actions_predictions.scores - prev_scores\\ndf_values[\\\"defensive_value\\\"] = df_actions_predictions.concedes - prev_concedes\\ndf_values[\\\"vaep\\\"] = df_values[\\\"offensive_value\\\"] + df_values[\\\"defensive_value\\\"]\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "dfs_values = []\n",
    "df_values = pd.DataFrame()\n",
    "\n",
    "sameteam = prev(df_actions_predictions.team_id) == df_actions_predictions.team_id\n",
    "prev_scores = prev(df_actions_predictions.scores) * sameteam + prev(df_actions_predictions.concedes) * (~sameteam)\n",
    "prev_concedes = prev(df_actions_predictions.concedes) * sameteam + prev(df_actions_predictions.scores) * (~sameteam)\n",
    "\n",
    "toolong_idx = abs(prev(df_actions_predictions.seconds_remaining) - df_actions_predictions.seconds_remaining) > 10\n",
    "prev_scores[toolong_idx] = 0\n",
    "prev_concedes[toolong_idx] = 0\n",
    "\n",
    "prevgoal_idx = prev(df_actions_predictions.event) == 'Goal'\n",
    "prev_scores[prevgoal_idx] = 0\n",
    "prev_concedes[prevgoal_idx] = 0\n",
    "\n",
    "df_values['offensive_value'] = df_actions_predictions.scores - prev_scores\n",
    "df_values['defensive_value'] = df_actions_predictions.concedes - prev_concedes\n",
    "df_values['vaep'] = df_values['offensive_value'] + df_values['defensive_value']"
   ]
  },
  {
   "source": [
    "## Analysing the values with focus on Zone Entries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 27;\n                var nbb_unformatted_code = \"df_final = pd.concat([df_actions_predictions,df_values],axis=1).dropna(subset=['vaep'])\\ndf_ranking = (df_final[['player','team','vaep']]\\n.groupby(['player','team'])\\n.agg(vaep_count=('vaep','count'),\\nvaep_mean=('vaep','mean'),\\nvaep_sum=('vaep','sum'))\\n.sort_values('vaep_sum',ascending=False)\\n.reset_index()\\n)\\n\\ndf_rank_events = (df_final[['event','vaep']]\\n.groupby(['event'])\\n.agg(vaep_count=('vaep','count'),\\nvaep_mean=('vaep','mean'),\\nvaep_sum=('vaep','sum'))\\n.sort_values('vaep_sum',ascending=False)\\n.reset_index()\\n)\\n\\ndf_zone_entries = (df_final.loc[womens['event']=='Zone Entry',['detail_1','defensive_value','offensive_value','vaep']]\\n.groupby(['detail_1'])\\n.agg(vaep_count=('vaep','count'),\\nvaep_mean=('vaep','mean'),\\nvaep_sum=('vaep','sum'))\\n.sort_values('vaep_sum',ascending=False)\\n.reset_index()\\n)\\n\\ndf_rank_strength = (df_final[['strength_state','event','vaep']]\\n.groupby(['strength_state','event'])\\n.agg(vaep_count=('vaep','count'),\\nvaep_mean=('vaep','mean'),\\nvaep_sum=('vaep','sum'))\\n.sort_values('vaep_sum',ascending=False)\\n.reset_index()\\n)\";\n                var nbb_formatted_code = \"df_final = pd.concat([df_actions_predictions, df_values], axis=1).dropna(\\n    subset=[\\\"vaep\\\"]\\n)\\ndf_ranking = (\\n    df_final[[\\\"player\\\", \\\"team\\\", \\\"vaep\\\"]]\\n    .groupby([\\\"player\\\", \\\"team\\\"])\\n    .agg(\\n        vaep_count=(\\\"vaep\\\", \\\"count\\\"),\\n        vaep_mean=(\\\"vaep\\\", \\\"mean\\\"),\\n        vaep_sum=(\\\"vaep\\\", \\\"sum\\\"),\\n    )\\n    .sort_values(\\\"vaep_sum\\\", ascending=False)\\n    .reset_index()\\n)\\n\\ndf_rank_events = (\\n    df_final[[\\\"event\\\", \\\"vaep\\\"]]\\n    .groupby([\\\"event\\\"])\\n    .agg(\\n        vaep_count=(\\\"vaep\\\", \\\"count\\\"),\\n        vaep_mean=(\\\"vaep\\\", \\\"mean\\\"),\\n        vaep_sum=(\\\"vaep\\\", \\\"sum\\\"),\\n    )\\n    .sort_values(\\\"vaep_sum\\\", ascending=False)\\n    .reset_index()\\n)\\n\\ndf_zone_entries = (\\n    df_final.loc[\\n        womens[\\\"event\\\"] == \\\"Zone Entry\\\",\\n        [\\\"detail_1\\\", \\\"defensive_value\\\", \\\"offensive_value\\\", \\\"vaep\\\"],\\n    ]\\n    .groupby([\\\"detail_1\\\"])\\n    .agg(\\n        vaep_count=(\\\"vaep\\\", \\\"count\\\"),\\n        vaep_mean=(\\\"vaep\\\", \\\"mean\\\"),\\n        vaep_sum=(\\\"vaep\\\", \\\"sum\\\"),\\n    )\\n    .sort_values(\\\"vaep_sum\\\", ascending=False)\\n    .reset_index()\\n)\\n\\ndf_rank_strength = (\\n    df_final[[\\\"strength_state\\\", \\\"event\\\", \\\"vaep\\\"]]\\n    .groupby([\\\"strength_state\\\", \\\"event\\\"])\\n    .agg(\\n        vaep_count=(\\\"vaep\\\", \\\"count\\\"),\\n        vaep_mean=(\\\"vaep\\\", \\\"mean\\\"),\\n        vaep_sum=(\\\"vaep\\\", \\\"sum\\\"),\\n    )\\n    .sort_values(\\\"vaep_sum\\\", ascending=False)\\n    .reset_index()\\n)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "df_final = pd.concat([df_actions_predictions,df_values],axis=1).dropna(subset=['vaep'])\n",
    "df_ranking = (df_final[['player','team','vaep']]\n",
    ".groupby(['player','team'])\n",
    ".agg(vaep_count=('vaep','count'),\n",
    "vaep_mean=('vaep','mean'),\n",
    "vaep_sum=('vaep','sum'))\n",
    ".sort_values('vaep_sum',ascending=False)\n",
    ".reset_index()\n",
    ")\n",
    "\n",
    "df_rank_events = (df_final[['event','vaep']]\n",
    ".groupby(['event'])\n",
    ".agg(vaep_count=('vaep','count'),\n",
    "vaep_mean=('vaep','mean'),\n",
    "vaep_sum=('vaep','sum'))\n",
    ".sort_values('vaep_sum',ascending=False)\n",
    ".reset_index()\n",
    ")\n",
    "\n",
    "df_zone_entries = (df_final.loc[womens['event']=='Zone Entry',['detail_1','defensive_value','offensive_value','vaep']]\n",
    ".groupby(['detail_1'])\n",
    ".agg(vaep_count=('vaep','count'),\n",
    "vaep_mean=('vaep','mean'),\n",
    "vaep_sum=('vaep','sum'))\n",
    ".sort_values('vaep_sum',ascending=False)\n",
    ".reset_index()\n",
    ")\n",
    "\n",
    "df_rank_strength = (df_final[['strength_state','event','vaep']]\n",
    ".groupby(['strength_state','event'])\n",
    ".agg(vaep_count=('vaep','count'),\n",
    "vaep_mean=('vaep','mean'),\n",
    "vaep_sum=('vaep','sum'))\n",
    ".sort_values('vaep_sum',ascending=False)\n",
    ".reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                    player                             team  vaep_count  \\\n",
       "0          Natalie Spooner         Olympic (Women) - Canada         411   \n",
       "1         Rebecca Johnston         Olympic (Women) - Canada         686   \n",
       "2  Kendall Coyne Schofield  Olympic (Women) - United States         466   \n",
       "3        Christina Putigna                     Boston Pride         365   \n",
       "4            Hilary Knight  Olympic (Women) - United States         447   \n",
       "5          Meghara McManus                     Boston Pride         219   \n",
       "6           Meghan Lorence              Minnesota Whitecaps         229   \n",
       "7      Mikyla Grant-Mentis                      Toronto Six         422   \n",
       "8              Sarah Nurse         Olympic (Women) - Canada         395   \n",
       "9        Autumn MacDougall                   Buffalo Beauts         285   \n",
       "\n",
       "   vaep_mean  vaep_sum  \n",
       "0   0.011804  4.851544  \n",
       "1   0.005566  3.818475  \n",
       "2   0.007227  3.367658  \n",
       "3   0.008955  3.268504  \n",
       "4   0.006952  3.107503  \n",
       "5   0.014031  3.072847  \n",
       "6   0.012839  2.940089  \n",
       "7   0.006495  2.741091  \n",
       "8   0.006885  2.719612  \n",
       "9   0.009461  2.696465  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>player</th>\n      <th>team</th>\n      <th>vaep_count</th>\n      <th>vaep_mean</th>\n      <th>vaep_sum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Natalie Spooner</td>\n      <td>Olympic (Women) - Canada</td>\n      <td>411</td>\n      <td>0.011804</td>\n      <td>4.851544</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Rebecca Johnston</td>\n      <td>Olympic (Women) - Canada</td>\n      <td>686</td>\n      <td>0.005566</td>\n      <td>3.818475</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Kendall Coyne Schofield</td>\n      <td>Olympic (Women) - United States</td>\n      <td>466</td>\n      <td>0.007227</td>\n      <td>3.367658</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Christina Putigna</td>\n      <td>Boston Pride</td>\n      <td>365</td>\n      <td>0.008955</td>\n      <td>3.268504</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Hilary Knight</td>\n      <td>Olympic (Women) - United States</td>\n      <td>447</td>\n      <td>0.006952</td>\n      <td>3.107503</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Meghara McManus</td>\n      <td>Boston Pride</td>\n      <td>219</td>\n      <td>0.014031</td>\n      <td>3.072847</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Meghan Lorence</td>\n      <td>Minnesota Whitecaps</td>\n      <td>229</td>\n      <td>0.012839</td>\n      <td>2.940089</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Mikyla Grant-Mentis</td>\n      <td>Toronto Six</td>\n      <td>422</td>\n      <td>0.006495</td>\n      <td>2.741091</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Sarah Nurse</td>\n      <td>Olympic (Women) - Canada</td>\n      <td>395</td>\n      <td>0.006885</td>\n      <td>2.719612</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Autumn MacDougall</td>\n      <td>Buffalo Beauts</td>\n      <td>285</td>\n      <td>0.009461</td>\n      <td>2.696465</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 29;\n                var nbb_unformatted_code = \"df_ranking.head(10)\";\n                var nbb_formatted_code = \"df_ranking.head(10)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "df_ranking.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             event  vaep_count  vaep_mean    vaep_sum\n",
       "0             Shot        3524   0.041261  145.404068\n",
       "1             Play       14673   0.003691   54.156487\n",
       "2             Goal         132   0.382158   50.444794\n",
       "3       Zone Entry        3744   0.005506   20.614611\n",
       "4         Takeaway        2092   0.000532    1.112332\n",
       "5      Dump In/Out        3545   0.000230    0.814852\n",
       "6    Penalty Taken         260   0.001924    0.500271\n",
       "7  Incomplete Play        6111  -0.007532  -46.025112\n",
       "8      Faceoff Win        1629  -0.030791  -50.159237\n",
       "9    Puck Recovery       15174  -0.007129 -108.171379"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>event</th>\n      <th>vaep_count</th>\n      <th>vaep_mean</th>\n      <th>vaep_sum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Shot</td>\n      <td>3524</td>\n      <td>0.041261</td>\n      <td>145.404068</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Play</td>\n      <td>14673</td>\n      <td>0.003691</td>\n      <td>54.156487</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Goal</td>\n      <td>132</td>\n      <td>0.382158</td>\n      <td>50.444794</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Zone Entry</td>\n      <td>3744</td>\n      <td>0.005506</td>\n      <td>20.614611</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Takeaway</td>\n      <td>2092</td>\n      <td>0.000532</td>\n      <td>1.112332</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Dump In/Out</td>\n      <td>3545</td>\n      <td>0.000230</td>\n      <td>0.814852</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Penalty Taken</td>\n      <td>260</td>\n      <td>0.001924</td>\n      <td>0.500271</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Incomplete Play</td>\n      <td>6111</td>\n      <td>-0.007532</td>\n      <td>-46.025112</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Faceoff Win</td>\n      <td>1629</td>\n      <td>-0.030791</td>\n      <td>-50.159237</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Puck Recovery</td>\n      <td>15174</td>\n      <td>-0.007129</td>\n      <td>-108.171379</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 30
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 30;\n                var nbb_unformatted_code = \"df_rank_events\";\n                var nbb_formatted_code = \"df_rank_events\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "df_rank_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  detail_1  vaep_count  vaep_mean   vaep_sum\n",
       "0  Carried        2316   0.007632  17.675030\n",
       "1   Played         261   0.006560   1.712269\n",
       "2   Dumped        1167   0.001052   1.227313"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>detail_1</th>\n      <th>vaep_count</th>\n      <th>vaep_mean</th>\n      <th>vaep_sum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Carried</td>\n      <td>2316</td>\n      <td>0.007632</td>\n      <td>17.675030</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Played</td>\n      <td>261</td>\n      <td>0.006560</td>\n      <td>1.712269</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dumped</td>\n      <td>1167</td>\n      <td>0.001052</td>\n      <td>1.227313</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 31
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 31;\n                var nbb_unformatted_code = \"df_zone_entries\";\n                var nbb_formatted_code = \"df_zone_entries\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "df_zone_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}